# AI Safety 

## Introduction
My interest in AI saftey began in December of 2025. After struggling to find things that I was really passionate in I was sent a job posting at Anthropic by a friend and started looking into AI alignment. I listened to a few podcasts from 80,000 Hrs and the AI X-risk Research Podcast. This sparked tons of interest for me and I started looking at what I could do to learn more. I am still really early stage in my learning but my commitment is high and my motivation strong. 

## Current Learning Path & Schedule
### *ðŸ”» Not started yet, ðŸŸ¡ In Progress, âœ… Done*
1. ðŸŸ¡ Educate myself as much as possible with podcasts, books, etc (ongoing)
2. ðŸŸ¡ BlueDot Impact AI Alignment course (Finish 02/28/2026)
3. ðŸŸ¡ Deep learning course from my university (BYU CS 474) (Finish 04/24/2026)
4. âœ… Build a small saftey wrapper for an LLM
5. ðŸ”» Contribute to an open-source Ai alignment project (I picked guardrails-ai) (Start 03/01/2026)
6. ðŸ”» Participate in an internship or fellowship in AI alignment (Start summer of 2026)

## Table of Contents
1. Safety wrapper for LLM API (quick project)
2. Kalman Filter (done for a previous project)
3. Conditioning & Stability (done for a previous project)
4. NEXT GOAL: Start contributing to an open-source AI Saftey Repo (looking at *guardrails-ai*)
